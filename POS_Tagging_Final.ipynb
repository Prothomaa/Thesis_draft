{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c943646f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9616432b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall size of my vocabulary = 24\n",
      "No. of times '<unk>' occurs in my vocabulary = 183\n",
      "No. of sentence = 20\n"
     ]
    }
   ],
   "source": [
    "#vocabulary generate kore pura training corpus er\n",
    "\n",
    "def generate_vocab(json_file_path, threshold=2):\n",
    "    with open(json_file_path, \"r\" , encoding=\"utf-8\") as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "\n",
    "    vocab = {'<unk>': 0}\n",
    "    counter = 0\n",
    "\n",
    "    # Counting the number of times each word occurs\n",
    "    for entry in json_data:\n",
    "        sentence = entry[\"sentence\"]\n",
    "        counter+=1\n",
    "\n",
    "        for word in sentence:\n",
    "            for subword in word.split():\n",
    "                if subword.strip():\n",
    "                    if subword not in vocab:\n",
    "                        vocab[subword] = 1\n",
    "                    else:\n",
    "                        vocab[subword] += 1\n",
    "\n",
    "    # Adding count of rare words to the count of <unk>\n",
    "    for k in list(vocab.keys())[1:]:\n",
    "        if vocab[k] < threshold:\n",
    "            vocab['<unk>'] += vocab[k]\n",
    "            del vocab[k]\n",
    "\n",
    "    # Sort vocab by count in descending order\n",
    "    sorted_vocab = {k: v for k, v in sorted(vocab.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "    # Push <unk> to the top\n",
    "    unk_val = sorted_vocab.pop('<unk>')\n",
    "    sorted_vocab = {'<unk>': unk_val, **sorted_vocab}\n",
    "\n",
    "    with open('vocab.txt', 'w', encoding=\"utf-8\") as f:\n",
    "        format_str = ''\n",
    "        for index, (key, count) in enumerate(sorted_vocab.items()):\n",
    "            format_str += key + '\\t' + str(index) + '\\t' + str(count) + '\\n'\n",
    "        f.write(format_str)\n",
    "\n",
    "    #print(\"Threshold =\", threshold)\n",
    "\n",
    "    return sorted_vocab, counter\n",
    "\n",
    "vocab, counter = generate_vocab(\"C:/Users/Admin/Desktop/Thesis/POS Tagging/archive/train_bangla.json\")\n",
    "print(\"Overall size of my vocabulary =\", len(vocab))\n",
    "print(\"No. of times '<unk>' occurs in my vocabulary =\", vocab['<unk>'])\n",
    "print(\"No. of sentence =\", counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ddf6977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of transition parameters = 96\n",
      "No. of emission parameters = 54\n"
     ]
    }
   ],
   "source": [
    "def load_training_data(file_path):\n",
    "    with open(file_path, 'r', encoding=\"utf-8\") as json_file:\n",
    "        return json.load(json_file)\n",
    "\n",
    "#transition holo NNP to VBP kotobar, emission holo \"Book\" word ta kotobar NNP r kotobar VBP\n",
    "#unique state holo distinct tag gulo niye banano list\n",
    "def process_training_data(data, vocab):\n",
    "    transition_probabilities = {}\n",
    "    emission_probabilities = {}\n",
    "    unique_states = []\n",
    "\n",
    "    for entry in data:\n",
    "        sentence_tokens = entry['sentence']\n",
    "        labels = entry['labels']\n",
    "        prior_state = 'head'  \n",
    "\n",
    "        for word, tag in zip(sentence_tokens, labels):\n",
    "            if word not in vocab.keys():\n",
    "                word = '<unk>'\n",
    "\n",
    "            # Update unique states\n",
    "            if tag not in unique_states:\n",
    "                unique_states.append(tag)\n",
    "\n",
    "            # Update transition count\n",
    "            transition_key = (prior_state, tag)\n",
    "            if transition_key not in transition_probabilities:\n",
    "                transition_probabilities[transition_key] = 1\n",
    "            else:\n",
    "                transition_probabilities[transition_key] += 1\n",
    "\n",
    "            # Update emission count\n",
    "            emission_key = (tag, word)\n",
    "            if emission_key not in emission_probabilities:\n",
    "                emission_probabilities[emission_key] = 1\n",
    "            else:\n",
    "                emission_probabilities[emission_key] += 1\n",
    "\n",
    "            prior_state = tag\n",
    "\n",
    "    return transition_probabilities, emission_probabilities, unique_states\n",
    "\n",
    "def normalize_probabilities(probabilities):\n",
    "    normalized_probabilities = {}\n",
    "    for key in probabilities:\n",
    "        state, value = key\n",
    "        total = sum(v for k, v in probabilities.items() if k[0] == state)\n",
    "        normalized_probabilities[key] = probabilities[key] / total\n",
    "    return normalized_probabilities\n",
    "\n",
    "#transition r emission gulo k dictionary er moto kore dekhano hoyese\n",
    "def save_hmm_model(transition_probs, emission_probs, output_file):\n",
    "    \n",
    "    # Convert tuple keys to strings\n",
    "    transition_probs = {str(k): v for k, v in transition_probs.items()}\n",
    "    emission_probs = {str(k): v for k, v in emission_probs.items()}\n",
    "\n",
    "    hmm_model = {\n",
    "        'transition': transition_probs,\n",
    "        'emission': emission_probs,\n",
    "    }\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(hmm_model, f)\n",
    "\n",
    "    return transition_probs, emission_probs\n",
    "\n",
    "training_data = load_training_data('C:/Users/Admin/Desktop/Thesis/POS Tagging/archive/train_bangla.json')\n",
    "transition_probs, emission_probs, unique_states = process_training_data(training_data, vocab)\n",
    "normalized_transition_probs = normalize_probabilities(transition_probs)\n",
    "normalized_emission_probs = normalize_probabilities(emission_probs)\n",
    "transition_param_dict, emission_param_dict = save_hmm_model(normalized_transition_probs, normalized_emission_probs, 'hmm.json')\n",
    "\n",
    "print(\"No. of transition parameters =\", len(transition_param_dict))\n",
    "print(\"No. of emission parameters =\", len(emission_param_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e77ba2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc99abdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#user_text = input(\"Enter your sentence: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5c305a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New list: [('আমি', 'PPR'), ('ভাত', 'NC'), ('খাই', 'VM'), ('।', 'VAUX')]\n",
      "Original list: [('সে', 'PPR'), ('বাজারে', 'NC'), ('যায়', 'VM'), ('।', 'VAUX'), ('তিনি', 'PPR'), ('কি', 'PWH'), ('সত্যিই', 'AMN'), ('ভালো', 'JJ'), ('মানুষ', 'NC'), ('?', 'PU'), ('সে', 'PPR'), ('ধনী', 'NC'), ('কিন্তু', 'CSB'), ('তার', 'PPR'), ('ভাই', 'NC'), ('গরীব', 'NC'), ('।', 'PU')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from bnlp import BengaliPOS\n",
    "\n",
    "bn_pos = BengaliPOS()\n",
    "\n",
    "#text = \"আমি ভাত খাই।\" \n",
    "text = \"আমি ভাত খাই। সে বাজারে যায়। তিনি কি সত্যিই ভালো মানুষ? সে ধনী কিন্তু তার ভাই গরীব।\"\n",
    "res_p = bn_pos.tag(text)\n",
    "#print(len(res_p))\n",
    "#print(res_p[3][0])\n",
    "\n",
    "# Example list of tuples\n",
    "def transfer_and_delete(original_list, specific_character):\n",
    "    new_list = []\n",
    "    i = 0\n",
    "    while i < len(original_list):\n",
    "        if original_list[i][0] != specific_character:\n",
    "            new_list.append(original_list[i])\n",
    "            del original_list[i]\n",
    "        else:\n",
    "            new_list.append(original_list[i])\n",
    "            del original_list[i]\n",
    "            break \n",
    "    return new_list\n",
    "\n",
    "# Example list of tuples\n",
    "original_list =res_p\n",
    "specific_character = \"।\"\n",
    "\n",
    "# Transfer elements until specific character is encountered\n",
    "res = transfer_and_delete(original_list, specific_character)\n",
    "\n",
    "print(\"New list:\", res)\n",
    "print(\"Original list:\", original_list)\n",
    "\n",
    "\n",
    "#print(res_p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7863ea03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom bnlp import NLTKTokenizer\\n\\nbnltk = NLTKTokenizer()\\n\\n#text = user_text\\ntext = \"আমি ভাত খাই। সে বাজারে যায়। তিনি কি সত্যিই ভালো মানুষ? সে ধনী কিন্তু তার ভাই গরীব।\"\\nsentence_tokens = bnltk.sentence_tokenize(text)\\nprint(len(sentence_tokens))\\nword_tokens = bnltk.word_tokenize(sentence_tokens[0])\\nprint(word_tokens)\\nprint(sentence_tokens)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from bnlp import NLTKTokenizer\n",
    "\n",
    "bnltk = NLTKTokenizer()\n",
    "\n",
    "#text = user_text\n",
    "text = \"আমি ভাত খাই। সে বাজারে যায়। তিনি কি সত্যিই ভালো মানুষ? সে ধনী কিন্তু তার ভাই গরীব।\"\n",
    "sentence_tokens = bnltk.sentence_tokenize(text)\n",
    "print(len(sentence_tokens))\n",
    "word_tokens = bnltk.word_tokenize(sentence_tokens[0])\n",
    "print(word_tokens)\n",
    "print(sentence_tokens)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68685afb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom bnlp import BengaliPOS\\n\\nbn_pos = BengaliPOS()\\npos = word_tokens \\nres = []\\nx = 0\\nfor i in pos:\\n    res.insert(x, bn_pos.tag(i))\\n    x = x+1\\nprint(len(res))\\nprint(res[0])\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from bnlp import BengaliPOS\n",
    "\n",
    "bn_pos = BengaliPOS()\n",
    "pos = word_tokens \n",
    "res = []\n",
    "x = 0\n",
    "for i in pos:\n",
    "    res.insert(x, bn_pos.tag(i))\n",
    "    x = x+1\n",
    "print(len(res))\n",
    "print(res[0])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a34341b4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('আমি', 'PPR')\n",
      "('ভাত', 'NC')\n",
      "('খাই', 'VM')\n",
      "('।', 'VAUX')\n",
      "Transition_prob =  {1: 0.15, 2: 0.5333333333333333, 3: 0.17073170731707318, 4: 0.20833333333333334}\n",
      "Emission_prob =  {1: 0, 2: 0, 3: 0, 4: 0}\n"
     ]
    }
   ],
   "source": [
    "#POS tag kore each word er r accuracy dey\n",
    "def greedy_decoding(formatted_data, vocab, transition_param_dict, emission_param_dict, state_track):\n",
    "    accuracy = []\n",
    "    transition_prob ={}\n",
    "    emission_prob = {}\n",
    "    max_stt = {}\n",
    "\n",
    "    prior_st = 'head'\n",
    "    counter = 0\n",
    "    for i in formatted_data:\n",
    "        print(i)\n",
    "        counter = counter+1\n",
    "        transition_prob[counter]=transition_param_dict.get(str((prior_st, i[1])), 1e-7)\n",
    "        emission_prob[counter]=emission_param_dict.get(str((i[1], i[0])), 0)\n",
    "        prior_st = i[1]\n",
    "\n",
    "    return transition_prob, emission_prob\n",
    "\n",
    "\n",
    "transition_prob, emission_prob = greedy_decoding(res, vocab, transition_param_dict, emission_param_dict, unique_states)\n",
    "print('Transition_prob = ', transition_prob)\n",
    "print('Emission_prob = ', emission_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b2b8a1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## eita nibo\n",
    "def reverse_graph(G):\n",
    "    '''Return the reversed graph g[dst][src]=G[src][dst]'''\n",
    "    g = {}\n",
    "    for src in G.keys():\n",
    "        for dst in G[src].keys():\n",
    "            if dst not in g.keys():\n",
    "                g[dst] = {}\n",
    "            g[dst][src] = G[src][dst]\n",
    "    return g\n",
    "\n",
    "\n",
    "def build_max(rg, root):\n",
    "    '''Find the max in-edge for every node except for the root.'''\n",
    "    mg = {}\n",
    "    for dst in rg.keys():\n",
    "        if dst == root:\n",
    "            continue\n",
    "        max_ind = -100\n",
    "        max_value = -100\n",
    "        for src in rg[dst].keys():\n",
    "            if rg[dst][src] >= max_value:\n",
    "                max_ind = src\n",
    "                max_value = rg[dst][src]\n",
    "        mg[dst] = {max_ind: max_value}\n",
    "    return mg\n",
    "\n",
    "\n",
    "def find_circle(mg):\n",
    "    '''Return the first circle if find, otherwise return None'''\n",
    "\n",
    "    for start in mg.keys():\n",
    "        visited = []\n",
    "        stack = [start]\n",
    "        while stack:\n",
    "            n = stack.pop()\n",
    "            if n in visited:\n",
    "                C = []\n",
    "                while n not in C:\n",
    "                    C.append(n)\n",
    "                    n = list(mg[n].keys())[0]\n",
    "                return C\n",
    "            visited.append(n)\n",
    "            if n in mg.keys():\n",
    "                stack.extend(list(mg[n].keys()))\n",
    "    return None\n",
    "\n",
    "\n",
    "def chu_liu_edmond(G, root):\n",
    "    ''' G: dict of dict of weights\n",
    "            G[i][j] = w means the edge from node i to node j has weight w.\n",
    "        root: the root node, has outgoing edges only.\n",
    "    '''\n",
    "    # reversed graph rg[dst][src] = G[src][dst]\n",
    "    rg = reverse_graph(G)\n",
    "    # root er only out edge\n",
    "    rg[root] = {}\n",
    "    # the maximum edge select korlam for each node other than root\n",
    "    mg = build_max(rg, root)\n",
    "\n",
    "    # check if mg is a tree (contains a circle)\n",
    "    C = find_circle(mg)\n",
    "    # circle na thakle, mg tai max_spanning_tree\n",
    "    if not C:\n",
    "        return reverse_graph(mg)\n",
    "\n",
    "    # jesob node circle kore tader k niye compact node korlm\n",
    "    all_nodes = G.keys()\n",
    "    vc = max(all_nodes) + 1\n",
    "\n",
    "    # new graph holo G_prime\n",
    "    V_prime = list(set(all_nodes) - set(C)) + [vc]\n",
    "    G_prime = {}\n",
    "    vc_in_idx = {}\n",
    "    vc_out_idx = {}\n",
    "    # Now add the edges to G_prime\n",
    "    for u in all_nodes:\n",
    "        for v in G[u].keys():\n",
    "            # incoming edge er weight calculation\n",
    "            if (u not in C) and (v in C):\n",
    "                if u not in G_prime.keys():\n",
    "                    G_prime[u] = {}\n",
    "                w = G[u][v] - list(mg[v].values())[0]\n",
    "                if (vc not in G_prime[u]) or (vc in G_prime[u] and w > G_prime[u][vc]):\n",
    "                    G_prime[u][vc] = w\n",
    "                    vc_in_idx[u] = v\n",
    "\n",
    "            # outgoing edge er weight calculation\n",
    "            elif (u in C) and (v not in C):\n",
    "                if vc not in G_prime.keys():\n",
    "                    G_prime[vc] = {}\n",
    "                w = G[u][v]\n",
    "                if (v not in G_prime[vc]) or (v in G_prime[vc] and w > G_prime[vc][v]):\n",
    "                    G_prime[vc][v] = w\n",
    "                    vc_out_idx[v] = u\n",
    "\n",
    "            # Third case: if the source and dest are all not in the circle, then just add the edge to the new graph.\n",
    "            elif (u not in C) and (v not in C):\n",
    "                if u not in G_prime.keys():\n",
    "                    G_prime[u] = {}\n",
    "                G_prime[u][v] = G[u][v]\n",
    "\n",
    "    # Recursively run the algorihtm on the new graph G_prime\n",
    "    A = chu_liu_edmond(G_prime, root)\n",
    "    # print(A)\n",
    "\n",
    "    # compacted node k vangbo, erpor incoming r outgoing edge gulo identify krbo\n",
    "    # always max ta choose krbo r bakigulo delete krbo\n",
    "    all_nodes_A = list(A.keys())\n",
    "    for src in all_nodes_A:\n",
    "        # The number of out-edges varies, could be 0 or any number <=|V\\C|\n",
    "        if src == vc:\n",
    "            for node_in in A[src].keys():\n",
    "                orig_out = vc_out_idx[node_in]\n",
    "                if orig_out not in A.keys():\n",
    "                    A[orig_out] = {}\n",
    "                A[orig_out][node_in] = G[orig_out][node_in]\n",
    "        else:\n",
    "            #for dst in A[src]:\n",
    "            for dst in list(A[src].keys()):\n",
    "                # There must be only one in-edge to vc.\n",
    "                if dst == vc:\n",
    "                    orig_in = vc_in_idx[src]\n",
    "                    A[src][orig_in] = G[src][orig_in]\n",
    "                    del A[src][dst]\n",
    "    del A[vc]\n",
    "\n",
    "\n",
    "    for node in C:\n",
    "        if node != orig_in:\n",
    "            src = list(mg[node].keys())[0]\n",
    "            if src not in A.keys():\n",
    "                A[src] = {}\n",
    "            A[src][node] = mg[node][src]\n",
    "\n",
    "    return A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cf9ea059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['root', ('আমি', 'PPR'), ('ভাত', 'NC'), ('খাই', 'VM'), ('।', 'VAUX')]\n"
     ]
    }
   ],
   "source": [
    "res = ['root']+res\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f681ce42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "('আমি', 'PPR')\n",
      "('ভাত', 'NC')\n",
      "('খাই', 'VM')\n",
      "('।', 'VAUX')\n",
      "G = {0: {1: 0.15, 2: 0.5333333333333333, 3: 17.073170731707318, 4: 6.9444444444444455}, 1: {2: 0.5333333333333333, 3: 0.17073170731707318, 4: 0.20833333333333334}, 2: {1: 0.15, 3: 0.17073170731707318, 4: 0.20833333333333334}, 3: {1: 0.15, 2: 0.5333333333333333, 4: 0.20833333333333334}, 4: {1: 0.15, 2: 0.5333333333333333, 3: 0.17073170731707318}}\n",
      "DP = {4: {1: 0.15, 2: 0.5333333333333333}, 0: {3: 17.073170731707318, 4: 6.9444444444444455}}\n",
      "Edges of the graph: [(4, 1), (4, 2), (0, 3), (0, 4)]\n",
      "। --> আমি\n",
      "। --> ভাত\n",
      "root --> খাই\n",
      "root --> ।\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nG3 = {}\\ndp = {}\\nG3= {0: {1: 0.13175059355956434, 2: 0.18211908429298618*10, 3: 1.0444370537508816, 4: 0.03227769819785418},\\n    1: {2: 0.18211908429298618, 3: 1.0444370537508816, 4: 0.03227769819785418},\\n    2: {1: 0.13175059355956434*3, 3: 1.0444370537508816*5, 4: 0.03227769819785418*2.5},\\n    3: {1: 0.13175059355956434, 2: 0.18211908429298618, 4: 0.03227769819785418*3},\\n    4: {1: 0.13175059355956434*2, 2: 0.18211908429298618*5, 3: 1.0444370537508816}}\\n#print(\"G3 :\", G3[1][2])\\ndp = chu_liu_edmond(G3, 0)\\nprint(dp)\\nedges = get_edges(dp)\\nprint(\"Edges of the graph:\", edges)\\nfor i in edges:\\n    x = i[0]-1\\n    y = i[1]-1\\n    if x == -1:\\n        print(\\'root\\', \\'-->\\', formatted_dev_data[y])\\n    else:\\n        print(formatted_dev_data[x],\\'-->\\', formatted_dev_data[y])\\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_edges(graph):\n",
    "    edges = []\n",
    "    for node in graph:\n",
    "        for neighbor in graph[node]:\n",
    "            edges.append((node, neighbor))\n",
    "    return edges\n",
    "\n",
    "# eita nibo\n",
    "#num_vertices = len(res)+1\n",
    "#print(num_vertices)\n",
    "\n",
    "G = {}\n",
    "dp = {}\n",
    "print(len(res))\n",
    "for i in range(len(res)):\n",
    "    G[i] = {}\n",
    "    if i==0:\n",
    "        p=100\n",
    "        q=200\n",
    "        for j in range(1,len(res)):\n",
    "            print(res[j])\n",
    "            weight = transition_prob[j] + emission_prob[j] \n",
    "            #if p ==0:\n",
    "            if res[j][1] == 'VM' or res[j][1] == 'VAUX':\n",
    "                #print('#')\n",
    "                weight = p*(transition_prob[j] + emission_prob[j])\n",
    "                p = p/3\n",
    "            if res[j][1] == 'CCD' or res[j][1] == 'CSB':\n",
    "                weight = p*(transition_prob[j] + emission_prob[j])\n",
    "                p = p*3\n",
    "                \n",
    "            G[i][j] = weight\n",
    "        continue\n",
    "                \n",
    "    for j in range(len(res)):\n",
    "        if (j == 0):\n",
    "            continue\n",
    "        if (i == j):\n",
    "            continue\n",
    "        weight = transition_prob[j] + emission_prob[j] \n",
    "        \n",
    "        G[i][j] = weight\n",
    "        \n",
    "print(\"G =\", G)\n",
    "\n",
    "dp = chu_liu_edmond(G, 0)\n",
    "print('DP =',dp)\n",
    "edges = get_edges(dp)\n",
    "print(\"Edges of the graph:\", edges)\n",
    "for i in edges:\n",
    "    #print(i)\n",
    "    x = i[0]\n",
    "    y = i[1]\n",
    "    if x == 0:\n",
    "        print('root', '-->', res[y][0])\n",
    "    else:\n",
    "        print(res[x][0],'-->', res[y][0])\n",
    "'''\n",
    "G3 = {}\n",
    "dp = {}\n",
    "G3= {0: {1: 0.13175059355956434, 2: 0.18211908429298618*10, 3: 1.0444370537508816, 4: 0.03227769819785418},\n",
    "    1: {2: 0.18211908429298618, 3: 1.0444370537508816, 4: 0.03227769819785418},\n",
    "    2: {1: 0.13175059355956434*3, 3: 1.0444370537508816*5, 4: 0.03227769819785418*2.5},\n",
    "    3: {1: 0.13175059355956434, 2: 0.18211908429298618, 4: 0.03227769819785418*3},\n",
    "    4: {1: 0.13175059355956434*2, 2: 0.18211908429298618*5, 3: 1.0444370537508816}}\n",
    "#print(\"G3 :\", G3[1][2])\n",
    "dp = chu_liu_edmond(G3, 0)\n",
    "print(dp)\n",
    "edges = get_edges(dp)\n",
    "print(\"Edges of the graph:\", edges)\n",
    "for i in edges:\n",
    "    x = i[0]-1\n",
    "    y = i[1]-1\n",
    "    if x == -1:\n",
    "        print('root', '-->', formatted_dev_data[y])\n",
    "    else:\n",
    "        print(formatted_dev_data[x],'-->', formatted_dev_data[y])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fcf7a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r\n"
     ]
    }
   ],
   "source": [
    "print(res[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4280ed3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"('head', 'RDF')\": 0.1, \"('RDF', 'NC')\": 1.0, \"('NC', 'NC')\": 0.25609756097560976, \"('NC', 'JJ')\": 0.08536585365853659, \"('JJ', 'NC')\": 0.8, \"('NC', 'CCD')\": 0.024390243902439025, \"('CCD', 'NC')\": 0.4444444444444444, \"('NC', 'NV')\": 0.04878048780487805, \"('NV', 'PU')\": 0.2, \"('NC', 'NP')\": 0.024390243902439025, \"('NP', 'JJ')\": 0.1, \"('NC', 'VM')\": 0.17073170731707318, \"('VM', 'JQ')\": 0.041666666666666664, \"('JQ', 'JJ')\": 0.3076923076923077, \"('NC', 'PP')\": 0.012195121951219513, \"('PP', 'JJ')\": 0.3333333333333333, \"('VM', 'PU')\": 0.5, \"('head', 'JQ')\": 0.05, \"('NC', 'PPR')\": 0.06097560975609756, \"('PPR', 'NC')\": 0.5333333333333333, \"('VM', 'VAUX')\": 0.20833333333333334, \"('VAUX', 'CCD')\": 0.4, \"('CCD', 'PPR')\": 0.3333333333333333, \"('NC', 'NST')\": 0.036585365853658534, \"('NST', 'JJ')\": 0.3333333333333333, \"('VAUX', 'PU')\": 0.6, \"('head', 'JJ')\": 0.1, \"('NV', 'CCD')\": 0.2, \"('NC', 'PU')\": 0.18292682926829268, \"('PU', 'NC')\": 0.5, \"('NV', 'JJ')\": 0.2, \"('JJ', 'VM')\": 0.15, \"('VM', 'PRL')\": 0.041666666666666664, \"('PRL', 'AMN')\": 1.0, \"('AMN', 'NC')\": 0.5, \"('head', 'NC')\": 0.2, \"('NP', 'CCD')\": 0.1, \"('NST', 'NP')\": 0.3333333333333333, \"('NP', 'PU')\": 0.7, \"('PU', 'NP')\": 0.23076923076923078, \"('PU', 'PPR')\": 0.07692307692307693, \"('NC', 'CCL')\": 0.012195121951219513, \"('CCL', 'NV')\": 1.0, \"('NV', 'VM')\": 0.4, \"('head', 'CSB')\": 0.15, \"('CSB', 'NC')\": 0.6666666666666666, \"('NC', 'JQ')\": 0.06097560975609756, \"('JQ', 'NC')\": 0.46153846153846156, \"('NC', 'DAB')\": 0.012195121951219513, \"('DAB', 'NC')\": 0.5, \"('PU', 'DAB')\": 0.07692307692307693, \"('DAB', 'PU')\": 0.5, \"('NP', 'NC')\": 0.1, \"('VM', 'CCD')\": 0.041666666666666664, \"('CCD', 'JQ')\": 0.1111111111111111, \"('JQ', 'JQ')\": 0.07692307692307693, \"('JQ', 'PU')\": 0.07692307692307693, \"('PU', 'JQ')\": 0.038461538461538464, \"('VM', 'CX')\": 0.041666666666666664, \"('CX', 'PU')\": 1.0, \"('head', 'EX')\": 0.05, \"('EX', 'MD')\": 1.0, \"('MD', 'VB')\": 1.0, \"('VB', 'NNS')\": 1.0, \"('NNS', 'VBG')\": 1.0, \"('VBG', 'WP')\": 1.0, \"('WP', 'PRP')\": 1.0, \"('PRP', 'VBD')\": 1.0, \"('VBD', '.')\": 1.0, '(\\'.\\', \"\\'\\'\")': 1.0, \"('PPR', 'JJ')\": 0.06666666666666667, \"('VM', 'JJ')\": 0.041666666666666664, \"('JJ', 'PP')\": 0.05, \"('PP', 'JQ')\": 0.3333333333333333, \"('PU', 'JJ')\": 0.038461538461538464, \"('head', 'CCD')\": 0.05, \"('PPR', 'PP')\": 0.06666666666666667, \"('PP', 'PPR')\": 0.3333333333333333, \"('JQ', 'VM')\": 0.07692307692307693, \"('head', 'PPR')\": 0.15, \"('PPR', 'JQ')\": 0.13333333333333333, \"('VM', 'PPR')\": 0.041666666666666664, \"('head', 'ALC')\": 0.05, \"('ALC', 'NC')\": 0.5, \"('head', 'VM')\": 0.05, \"('PPR', 'VM')\": 0.13333333333333333, \"('NC', 'ALC')\": 0.012195121951219513, \"('ALC', 'PU')\": 0.5, \"('NST', 'PU')\": 0.3333333333333333, \"('head', 'NP')\": 0.05, \"('PU', 'CCD')\": 0.038461538461538464, \"('CCD', 'PU')\": 0.1111111111111111, \"('VM', 'NC')\": 0.041666666666666664, \"('CSB', 'DAB')\": 0.3333333333333333, \"('PPR', 'AMN')\": 0.06666666666666667, \"('AMN', 'VM')\": 0.5}\n"
     ]
    }
   ],
   "source": [
    "print(transition_param_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7291f3b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"('RDF', '<unk>')\": 1.0, \"('NC', '<unk>')\": 0.9390243902439024, \"('JJ', '<unk>')\": 0.8, \"('CCD', 'এবং')\": 0.6666666666666666, \"('NV', '<unk>')\": 0.6, \"('PU', '।')\": 0.35555555555555557, \"('NP', '<unk>')\": 0.7, \"('JJ', 'রাজনৈতিক')\": 0.1, \"('VM', '<unk>')\": 0.8333333333333334, \"('JQ', '<unk>')\": 1.0, \"('PP', '<unk>')\": 1.0, \"('VM', 'করে')\": 0.125, \"('NC', 'স্থান')\": 0.024390243902439025, \"('PPR', '<unk>')\": 0.6, \"('VAUX', '<unk>')\": 0.8, \"('NST', 'মধ্যে')\": 0.6666666666666666, \"('NV', 'চালানো')\": 0.4, \"('CCD', 'ও')\": 0.2222222222222222, \"('PU', '-')\": 0.08888888888888889, \"('PRL', '<unk>')\": 1.0, \"('AMN', '<unk>')\": 1.0, \"('NP', 'আম')\": 0.1, \"('PU', ',')\": 0.4, \"('PPR', 'তারা')\": 0.13333333333333333, \"('CCL', '<unk>')\": 1.0, \"('CSB', 'কিন্তু')\": 0.6666666666666666, \"('DAB', 'এ')\": 0.75, \"('NP', 'সি')\": 0.2, \"('CX', '<unk>')\": 1.0, \"('EX', '<unk>')\": 1.0, \"('MD', '<unk>')\": 1.0, \"('VB', '<unk>')\": 1.0, \"('NNS', 'ও')\": 1.0, \"('VBG', '<unk>')\": 1.0, \"('WP', '<unk>')\": 1.0, \"('PRP', '<unk>')\": 1.0, \"('VBD', '<unk>')\": 1.0, \"('.', '<unk>')\": 1.0, '(\"\\'\\'\", \\'<unk>\\')': 1.0, \"('JJ', 'বৃত্তিমূলক')\": 0.1, \"('NC', 'শিক্ষার')\": 0.024390243902439025, \"('VM', 'হবে')\": 0.041666666666666664, \"('CCD', '<unk>')\": 0.1111111111111111, \"('ALC', '<unk>')\": 1.0, \"('PU', '!')\": 0.044444444444444446, \"('PU', '<unk>')\": 0.022222222222222223, \"('PPR', 'তার')\": 0.13333333333333333, \"('PPR', 'তাকে')\": 0.13333333333333333, \"('NST', '<unk>')\": 0.3333333333333333, '(\\'PU\\', \"\\'\")': 0.08888888888888889, \"('NC', 'আম')\": 0.012195121951219513, \"('CSB', '<unk>')\": 0.3333333333333333, \"('DAB', 'এই')\": 0.25, \"('VAUX', 'হবে')\": 0.2}\n"
     ]
    }
   ],
   "source": [
    "print(emission_param_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5b1fe13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RDF', 'NC', 'JJ', 'CCD', 'NV', 'PU', 'NP', 'VM', 'JQ', 'PP', 'PPR', 'VAUX', 'NST', 'PRL', 'AMN', 'CCL', 'CSB', 'DAB', 'CX', 'EX', 'MD', 'VB', 'NNS', 'VBG', 'WP', 'PRP', 'VBD', '.', \"''\", 'ALC']\n"
     ]
    }
   ],
   "source": [
    "print(unique_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06b34f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install Flask flask-ngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e1edcd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [16/Feb/2024 15:24:37] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [16/Feb/2024 15:24:41] \"POST / HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template, request\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Example NLP function\n",
    "def analyze_text(text):\n",
    "    # Replace this with your actual NLP processing logic\n",
    "    # For demonstration, let's just return the input text\n",
    "    return {\"input\": text, \"analysis\": \"Placeholder analysis\"}\n",
    "\n",
    "@app.route(\"/\", methods=['GET', 'POST'])\n",
    "def index():\n",
    "    if request.method == 'POST':\n",
    "        user_input = request.form['user_input']\n",
    "        analysis = analyze_text(user_input)\n",
    "        return render_template('new_result.html', analysis=analysis)\n",
    "    return render_template('new.html')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28847cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bdeb9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

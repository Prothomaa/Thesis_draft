{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec7aa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01991e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocabulary generate kore pura training corpus er\n",
    "\n",
    "def generate_vocab(json_file_path, threshold=2):\n",
    "    with open(json_file_path, \"r\" , encoding=\"utf-8\") as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "\n",
    "    vocab = {'<unk>': 0}\n",
    "    counter = 0\n",
    "\n",
    "    # each word occurs count kore\n",
    "    for entry in json_data:\n",
    "        sentence = entry[\"sentence\"]\n",
    "        counter+=1\n",
    "\n",
    "        for word in sentence:\n",
    "            for subword in word.split():\n",
    "                if subword.strip():\n",
    "                    if subword not in vocab:\n",
    "                        vocab[subword] = 1\n",
    "                    else:\n",
    "                        vocab[subword] += 1\n",
    "\n",
    "    # jegulo dataset e 1 bar hoese segulo count kore\n",
    "    for k in list(vocab.keys())[1:]:\n",
    "        if vocab[k] < threshold:\n",
    "            vocab['<unk>'] += vocab[k]\n",
    "            del vocab[k]\n",
    "    #vocab['<unk>'] = 0\n",
    "    # descending order e vocab sort kore\n",
    "    sorted_vocab = {k: v for k, v in sorted(vocab.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "    # <unk> k top e push kore\n",
    "    unk_val = sorted_vocab.pop('<unk>')\n",
    "    sorted_vocab = {'<unk>': unk_val, **sorted_vocab}\n",
    "\n",
    "    with open('vocab.txt', 'w', encoding=\"utf-8\") as f:\n",
    "        format_str = ''\n",
    "        for index, (key, count) in enumerate(sorted_vocab.items()):\n",
    "            format_str += key + '\\t' + str(index) + '\\t' + str(count) + '\\n'\n",
    "        f.write(format_str)\n",
    "\n",
    "    #print(\"Threshold =\", threshold)\n",
    "\n",
    "    return sorted_vocab, counter\n",
    "\n",
    "vocab, counter = generate_vocab(\"C:/Users/Admin/Desktop/Thesis/POS Tagging/archive/train_bangla.json\")\n",
    "print(\"Overall size of my vocabulary =\", len(vocab))\n",
    "#print(\"No. of times '<unk>' occurs in my vocabulary =\", vocab['<unk>'])\n",
    "print(\"No. of sentence =\", counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f268cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data(file_path):\n",
    "    with open(file_path, 'r', encoding=\"utf-8\") as json_file:\n",
    "        return json.load(json_file)\n",
    "\n",
    "#transition holo NNP to VBP kotobar, emission holo \"Book\" word ta kotobar NNP r kotobar VBP\n",
    "#unique state holo distinct tag gulo niye banano list\n",
    "def process_training_data(data, vocab):\n",
    "    transition_probabilities = {}\n",
    "    emission_probabilities = {}\n",
    "    unique_states = []\n",
    "\n",
    "    for entry in data:\n",
    "        sentence_tokens = entry['sentence']\n",
    "        labels = entry['labels']\n",
    "        prior_state = 'root'  \n",
    "\n",
    "        for word, tag in zip(sentence_tokens, labels):\n",
    "            if word not in vocab.keys():\n",
    "                word = '<unk>'\n",
    "\n",
    "            # Update unique states\n",
    "            if tag not in unique_states:\n",
    "                unique_states.append(tag)\n",
    "\n",
    "            # Update transition count\n",
    "            transition_key = (prior_state, tag)\n",
    "            if transition_key not in transition_probabilities:\n",
    "                transition_probabilities[transition_key] = 1\n",
    "            else:\n",
    "                transition_probabilities[transition_key] += 1\n",
    "\n",
    "            # Update emission count\n",
    "            emission_key = (tag, word)\n",
    "            if emission_key not in emission_probabilities:\n",
    "                emission_probabilities[emission_key] = 1\n",
    "            else:\n",
    "                emission_probabilities[emission_key] += 1\n",
    "\n",
    "            prior_state = tag\n",
    "\n",
    "    return transition_probabilities, emission_probabilities, unique_states\n",
    "\n",
    "# normalize kore\n",
    "def normalize_probabilities(probabilities):\n",
    "    normalized_probabilities = {}\n",
    "    for key in probabilities:\n",
    "        state, value = key\n",
    "        total = sum(v for k, v in probabilities.items() if k[0] == state)\n",
    "        normalized_probabilities[key] = probabilities[key] / total\n",
    "    return normalized_probabilities\n",
    "\n",
    "#transition r emission gulo k dictionary er moto kore dekhano hoyese. hmm.json e ase sobgulo\n",
    "def save_hmm_model(transition_probs, emission_probs, output_file):\n",
    "    \n",
    "    # Convert tuple keys to strings\n",
    "    transition_probs = {str(k): v for k, v in transition_probs.items()}\n",
    "    emission_probs = {str(k): v for k, v in emission_probs.items()}\n",
    "\n",
    "    hmm_model = {\n",
    "        'transition': transition_probs,\n",
    "        'emission': emission_probs,\n",
    "    }\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(hmm_model, f)\n",
    "\n",
    "    return transition_probs, emission_probs\n",
    "\n",
    "training_data = load_training_data('C:/Users/Admin/Desktop/Thesis/POS Tagging/archive/train_bangla.json')\n",
    "transition_probs, emission_probs, unique_states = process_training_data(training_data, vocab)\n",
    "normalized_transition_probs = normalize_probabilities(transition_probs)\n",
    "normalized_emission_probs = normalize_probabilities(emission_probs)\n",
    "transition_param_dict, emission_param_dict = save_hmm_model(normalized_transition_probs, normalized_emission_probs, 'hmm.json')\n",
    "\n",
    "print(\"No. of transition parameters =\", len(transition_param_dict))\n",
    "print(\"No. of emission parameters =\", len(emission_param_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e59c69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f922e514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# formatted_data er transition r emission dey \n",
    "def greedy_decoding(formatted_data, vocab, transition_param_dict, emission_param_dict, state_track):\n",
    "    #accuracy = []\n",
    "    transition_prob ={}\n",
    "    emission_prob = {}\n",
    "    #max_stt = {}\n",
    "\n",
    "    prior_st = 'root'\n",
    "    counter = 0\n",
    "    for i in formatted_data:\n",
    "        print(i)\n",
    "        counter = counter+1\n",
    "        transition_prob[counter]=transition_param_dict.get(str((prior_st, i[1])), 1e-7)\n",
    "        emission_prob[counter]=emission_param_dict.get(str((i[1], i[0])), 0)\n",
    "        prior_st = i[1]\n",
    "\n",
    "    return transition_prob, emission_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb1fbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_graph(G):\n",
    "    '''reverse graph ta return kore like g[dst][src]=G[src][dst]'''\n",
    "    g = {}\n",
    "    for src in G.keys():\n",
    "        for dst in G[src].keys():\n",
    "            if dst not in g.keys():\n",
    "                g[dst] = {}\n",
    "            g[dst][src] = G[src][dst]\n",
    "    return g\n",
    "\n",
    "\n",
    "def build_max(rg, root): # rg always reverse_graph hbe\n",
    "    '''max incoming edge find out kore each node er'''\n",
    "    mg = {}\n",
    "    for dst in rg.keys():\n",
    "        if dst == root: # root thakle ignore krbe\n",
    "            continue\n",
    "        max_ind = -100\n",
    "        max_value = -100\n",
    "        for src in rg[dst].keys():\n",
    "            if rg[dst][src] >= max_value:\n",
    "                max_ind = src\n",
    "                max_value = rg[dst][src]\n",
    "        mg[dst] = {max_ind: max_value}\n",
    "    return mg\n",
    "\n",
    "\n",
    "def find_circle(mg):\n",
    "    '''circle ase kina check kore DFS diye. na thakle None return kore'''\n",
    "\n",
    "    for start in mg.keys():\n",
    "        visited = [] # jegulo visited hoye giyese segulo ekhane thakbe\n",
    "        stack = [start]\n",
    "        while stack: #stack empty na hoa prjnto traverse cholbe\n",
    "            n = stack.pop()\n",
    "            if n in visited:# n jodi already visited hoy tahole cycle\n",
    "                C = [] # empty list declare krlm\n",
    "                while n not in C: # ekhane backtrack er kaj ta hoy loop diye\n",
    "                    C.append(n) # n k C er moddhe rakhlm\n",
    "                    n = list(mg[n].keys())[0] # n k update krbo tar parent node diye \n",
    "                return C # Cycle er node gulor list return kore\n",
    "            visited.append(n)\n",
    "            if n in mg.keys(): # n er kono child ase kina check kore\n",
    "                stack.extend(list(mg[n].keys())) # stack er moddhe current node n er child k rakhe. etai DFS er condition\n",
    "    return None # cycle na thakle None return kore\n",
    "\n",
    "\n",
    "def chu_liu_edmond(G, root):\n",
    "    \n",
    "    # reversed graph rg[dst][src] = G[src][dst]\n",
    "    rg = reverse_graph(G)\n",
    "    # root er only out edge\n",
    "    rg[root] = {}\n",
    "    # the maximum edge select korlam for each node other than root\n",
    "    mg = build_max(rg, root)\n",
    "\n",
    "    # check if mg is a tree (contains a circle)\n",
    "    C = find_circle(mg)\n",
    "    # circle na thakle, mg tai max_spanning_tree\n",
    "    if not C:\n",
    "        return reverse_graph(mg)\n",
    "\n",
    "    # jesob node circle kore tader k niye compact node korlm\n",
    "    all_nodes = G.keys()\n",
    "    vc = max(all_nodes) + 1\n",
    "\n",
    "    # new graph holo G_prime\n",
    "    #V_prime = list(set(all_nodes) - set(C)) + [vc]\n",
    "    G_prime = {}\n",
    "    vc_in_idx = {}\n",
    "    vc_out_idx = {}\n",
    "    # Now add the edges to G_prime\n",
    "    for u in all_nodes:\n",
    "        for v in G[u].keys():\n",
    "            # incoming edge er weight calculation. subtraction hoy\n",
    "            if (u not in C) and (v in C):\n",
    "                if u not in G_prime.keys():\n",
    "                    G_prime[u] = {}\n",
    "                w = G[u][v] - list(mg[v].values())[0] \n",
    "                if (vc not in G_prime[u]) or (vc in G_prime[u] and w > G_prime[u][vc]):\n",
    "                    G_prime[u][vc] = w\n",
    "                    vc_in_idx[u] = v\n",
    "\n",
    "            # outgoing edge er weight calculation. jeta max oita nibo\n",
    "            elif (u in C) and (v not in C):\n",
    "                if vc not in G_prime.keys():\n",
    "                    G_prime[vc] = {}\n",
    "                w = G[u][v]\n",
    "                if (v not in G_prime[vc]) or (v in G_prime[vc] and w > G_prime[vc][v]):\n",
    "                    G_prime[vc][v] = w\n",
    "                    vc_out_idx[v] = u\n",
    "\n",
    "            # Third case: if the source and dest are all not in the circle, then just add the edge to the new graph.\n",
    "            elif (u not in C) and (v not in C):\n",
    "                if u not in G_prime.keys():\n",
    "                    G_prime[u] = {}\n",
    "                G_prime[u][v] = G[u][v]\n",
    "\n",
    "    # Recursively run the algorihtm on the new graph G_prime\n",
    "    A = chu_liu_edmond(G_prime, root)\n",
    "    # print(A)\n",
    "\n",
    "    # compacted node k vangbo, erpor incoming r outgoing edge gulo identify krbo\n",
    "    # always max ta choose krbo r bakigulo delete krbo\n",
    "    all_nodes_A = list(A.keys())\n",
    "    for src in all_nodes_A:\n",
    "        # The number of out-edges varies, could be 0 or any number <=|V\\C|\n",
    "        if src == vc:\n",
    "            for node_in in A[src].keys():\n",
    "                orig_out = vc_out_idx[node_in]\n",
    "                if orig_out not in A.keys():\n",
    "                    A[orig_out] = {}\n",
    "                A[orig_out][node_in] = G[orig_out][node_in]\n",
    "        else:\n",
    "            #for dst in A[src]:\n",
    "            for dst in list(A[src].keys()):\n",
    "                # There must be only one in-edge to vc.\n",
    "                if dst == vc:\n",
    "                    orig_in = vc_in_idx[src]\n",
    "                    A[src][orig_in] = G[src][orig_in]\n",
    "                    del A[src][dst]\n",
    "    \n",
    "    if vc in A:\n",
    "        del A[vc]\n",
    "\n",
    "    for node in C:\n",
    "        if node != orig_in:\n",
    "            src = list(mg[node].keys())[0]\n",
    "            if src not in A.keys():\n",
    "                A[src] = {}\n",
    "            A[src][node] = mg[node][src]\n",
    "\n",
    "    return A\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c943646f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9616432b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall size of my vocabulary = 61\n",
      "No. of times '<unk>' occurs in my vocabulary = 347\n",
      "No. of sentence = 51\n"
     ]
    }
   ],
   "source": [
    "#vocabulary generate kore pura training corpus er\n",
    "\n",
    "def generate_vocab(json_file_path, threshold=2):\n",
    "    with open(json_file_path, \"r\" , encoding=\"utf-8\") as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "\n",
    "    vocab = {'<unk>': 0}\n",
    "    counter = 0\n",
    "\n",
    "    # Counting the number of times each word occurs\n",
    "    for entry in json_data:\n",
    "        sentence = entry[\"sentence\"]\n",
    "        counter+=1\n",
    "\n",
    "        for word in sentence:\n",
    "            for subword in word.split():\n",
    "                if subword.strip():\n",
    "                    if subword not in vocab:\n",
    "                        vocab[subword] = 1\n",
    "                    else:\n",
    "                        vocab[subword] += 1\n",
    "\n",
    "    # Adding count of rare words to the count of <unk>\n",
    "    for k in list(vocab.keys())[1:]:\n",
    "        if vocab[k] < threshold:\n",
    "            vocab['<unk>'] += vocab[k]\n",
    "            del vocab[k]\n",
    "\n",
    "    # Sort vocab by count in descending order\n",
    "    sorted_vocab = {k: v for k, v in sorted(vocab.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "    # Push <unk> to the top\n",
    "    unk_val = sorted_vocab.pop('<unk>')\n",
    "    sorted_vocab = {'<unk>': unk_val, **sorted_vocab}\n",
    "\n",
    "    with open('vocab.txt', 'w', encoding=\"utf-8\") as f:\n",
    "        format_str = ''\n",
    "        for index, (key, count) in enumerate(sorted_vocab.items()):\n",
    "            format_str += key + '\\t' + str(index) + '\\t' + str(count) + '\\n'\n",
    "        f.write(format_str)\n",
    "\n",
    "    #print(\"Threshold =\", threshold)\n",
    "\n",
    "    return sorted_vocab, counter\n",
    "\n",
    "vocab, counter = generate_vocab(\"C:/Users/Admin/Desktop/Thesis/POS Tagging/archive/train_bangla.json\")\n",
    "print(\"Overall size of my vocabulary =\", len(vocab))\n",
    "print(\"No. of times '<unk>' occurs in my vocabulary =\", vocab['<unk>'])\n",
    "print(\"No. of sentence =\", counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ddf6977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of transition parameters = 144\n",
      "No. of emission parameters = 98\n"
     ]
    }
   ],
   "source": [
    "def load_training_data(file_path):\n",
    "    with open(file_path, 'r', encoding=\"utf-8\") as json_file:\n",
    "        return json.load(json_file)\n",
    "\n",
    "#transition holo NNP to VBP kotobar, emission holo \"Book\" word ta kotobar NNP r kotobar VBP\n",
    "#unique state holo distinct tag gulo niye banano list\n",
    "def process_training_data(data, vocab):\n",
    "    transition_probabilities = {}\n",
    "    emission_probabilities = {}\n",
    "    unique_states = []\n",
    "\n",
    "    for entry in data:\n",
    "        sentence_tokens = entry['sentence']\n",
    "        labels = entry['labels']\n",
    "        prior_state = 'root'  \n",
    "\n",
    "        for word, tag in zip(sentence_tokens, labels):\n",
    "            if word not in vocab.keys():\n",
    "                word = '<unk>'\n",
    "\n",
    "            # Update unique states\n",
    "            if tag not in unique_states:\n",
    "                unique_states.append(tag)\n",
    "\n",
    "            # Update transition count\n",
    "            transition_key = (prior_state, tag)\n",
    "            if transition_key not in transition_probabilities:\n",
    "                transition_probabilities[transition_key] = 1\n",
    "            else:\n",
    "                transition_probabilities[transition_key] += 1\n",
    "\n",
    "            # Update emission count\n",
    "            emission_key = (tag, word)\n",
    "            if emission_key not in emission_probabilities:\n",
    "                emission_probabilities[emission_key] = 1\n",
    "            else:\n",
    "                emission_probabilities[emission_key] += 1\n",
    "\n",
    "            prior_state = tag\n",
    "\n",
    "    return transition_probabilities, emission_probabilities, unique_states\n",
    "\n",
    "def normalize_probabilities(probabilities):\n",
    "    normalized_probabilities = {}\n",
    "    for key in probabilities:\n",
    "        state, value = key\n",
    "        total = sum(v for k, v in probabilities.items() if k[0] == state)\n",
    "        normalized_probabilities[key] = probabilities[key] / total\n",
    "    return normalized_probabilities\n",
    "\n",
    "#transition r emission gulo k dictionary er moto kore dekhano hoyese\n",
    "def save_hmm_model(transition_probs, emission_probs, output_file):\n",
    "    \n",
    "    # Convert tuple keys to strings\n",
    "    transition_probs = {str(k): v for k, v in transition_probs.items()}\n",
    "    emission_probs = {str(k): v for k, v in emission_probs.items()}\n",
    "\n",
    "    hmm_model = {\n",
    "        'transition': transition_probs,\n",
    "        'emission': emission_probs,\n",
    "    }\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(hmm_model, f)\n",
    "\n",
    "    return transition_probs, emission_probs\n",
    "\n",
    "training_data = load_training_data('C:/Users/Admin/Desktop/Thesis/POS Tagging/archive/train_bangla.json')\n",
    "transition_probs, emission_probs, unique_states = process_training_data(training_data, vocab)\n",
    "normalized_transition_probs = normalize_probabilities(transition_probs)\n",
    "normalized_emission_probs = normalize_probabilities(emission_probs)\n",
    "transition_param_dict, emission_param_dict = save_hmm_model(normalized_transition_probs, normalized_emission_probs, 'hmm.json')\n",
    "\n",
    "print(\"No. of transition parameters =\", len(transition_param_dict))\n",
    "print(\"No. of emission parameters =\", len(emission_param_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e77ba2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc99abdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#user_text = input(\"Enter your sentence: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26c24c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#POS tag kore each word er r accuracy dey\n",
    "def greedy_decoding(formatted_data, vocab, transition_param_dict, emission_param_dict, state_track):\n",
    "    accuracy = []\n",
    "    transition_prob ={}\n",
    "    emission_prob = {}\n",
    "    max_stt = {}\n",
    "\n",
    "    prior_st = 'root'\n",
    "    counter = 0\n",
    "    for i in formatted_data:\n",
    "        print(i)\n",
    "        counter = counter+1\n",
    "        transition_prob[counter]=transition_param_dict.get(str((prior_st, i[1])), 1e-7)\n",
    "        emission_prob[counter]=emission_param_dict.get(str((i[1], i[0])), 0)\n",
    "        prior_st = i[1]\n",
    "\n",
    "    return transition_prob, emission_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "527aa258",
   "metadata": {},
   "outputs": [],
   "source": [
    "## eita nibo\n",
    "\n",
    "def reverse_graph(G):\n",
    "    '''Return the reversed graph g[dst][src]=G[src][dst]'''\n",
    "    g = {}\n",
    "    for src in G.keys():\n",
    "        for dst in G[src].keys():\n",
    "            if dst not in g.keys():\n",
    "                g[dst] = {}\n",
    "            g[dst][src] = G[src][dst]\n",
    "    return g\n",
    "\n",
    "\n",
    "def build_max(rg, root):\n",
    "    '''Find the max in-edge for every node except for the root.'''\n",
    "    mg = {}\n",
    "    for dst in rg.keys():\n",
    "        if dst == root:\n",
    "            continue\n",
    "        max_ind = -100\n",
    "        max_value = -100\n",
    "        for src in rg[dst].keys():\n",
    "            if rg[dst][src] >= max_value:\n",
    "                max_ind = src\n",
    "                max_value = rg[dst][src]\n",
    "        mg[dst] = {max_ind: max_value}\n",
    "    return mg\n",
    "\n",
    "\n",
    "def find_circle(mg):\n",
    "    '''Return the first circle if find, otherwise return None'''\n",
    "\n",
    "    for start in mg.keys():\n",
    "        visited = []\n",
    "        stack = [start]\n",
    "        while stack:\n",
    "            n = stack.pop()\n",
    "            if n in visited:\n",
    "                C = []\n",
    "                while n not in C:\n",
    "                    C.append(n)\n",
    "                    n = list(mg[n].keys())[0]\n",
    "                return C\n",
    "            visited.append(n)\n",
    "            if n in mg.keys():\n",
    "                stack.extend(list(mg[n].keys()))\n",
    "    return None\n",
    "\n",
    "\n",
    "def chu_liu_edmond(G, root):\n",
    "    ''' G: dict of dict of weights\n",
    "            G[i][j] = w means the edge from node i to node j has weight w.\n",
    "        root: the root node, has outgoing edges only.\n",
    "    '''\n",
    "    # reversed graph rg[dst][src] = G[src][dst]\n",
    "    rg = reverse_graph(G)\n",
    "    # root er only out edge\n",
    "    rg[root] = {}\n",
    "    # the maximum edge select korlam for each node other than root\n",
    "    mg = build_max(rg, root)\n",
    "\n",
    "    # check if mg is a tree (contains a circle)\n",
    "    C = find_circle(mg)\n",
    "    # circle na thakle, mg tai max_spanning_tree\n",
    "    if not C:\n",
    "        return reverse_graph(mg)\n",
    "\n",
    "    # jesob node circle kore tader k niye compact node korlm\n",
    "    all_nodes = G.keys()\n",
    "    vc = max(all_nodes) + 1\n",
    "\n",
    "    # new graph holo G_prime\n",
    "    V_prime = list(set(all_nodes) - set(C)) + [vc]\n",
    "    G_prime = {}\n",
    "    vc_in_idx = {}\n",
    "    vc_out_idx = {}\n",
    "    # Now add the edges to G_prime\n",
    "    for u in all_nodes:\n",
    "        for v in G[u].keys():\n",
    "            # incoming edge er weight calculation\n",
    "            if (u not in C) and (v in C):\n",
    "                if u not in G_prime.keys():\n",
    "                    G_prime[u] = {}\n",
    "                w = G[u][v] - list(mg[v].values())[0] \n",
    "                if (vc not in G_prime[u]) or (vc in G_prime[u] and w > G_prime[u][vc]):\n",
    "                    G_prime[u][vc] = w\n",
    "                    vc_in_idx[u] = v\n",
    "\n",
    "            # outgoing edge er weight calculation\n",
    "            elif (u in C) and (v not in C):\n",
    "                if vc not in G_prime.keys():\n",
    "                    G_prime[vc] = {}\n",
    "                w = G[u][v]\n",
    "                if (v not in G_prime[vc]) or (v in G_prime[vc] and w > G_prime[vc][v]):\n",
    "                    G_prime[vc][v] = w\n",
    "                    vc_out_idx[v] = u\n",
    "\n",
    "            # Third case: if the source and dest are all not in the circle, then just add the edge to the new graph.\n",
    "            elif (u not in C) and (v not in C):\n",
    "                if u not in G_prime.keys():\n",
    "                    G_prime[u] = {}\n",
    "                G_prime[u][v] = G[u][v]\n",
    "\n",
    "    # Recursively run the algorihtm on the new graph G_prime\n",
    "    A = chu_liu_edmond(G_prime, root)\n",
    "    # print(A)\n",
    "\n",
    "    # compacted node k vangbo, erpor incoming r outgoing edge gulo identify krbo\n",
    "    # always max ta choose krbo r bakigulo delete krbo\n",
    "    all_nodes_A = list(A.keys())\n",
    "    for src in all_nodes_A:\n",
    "        # The number of out-edges varies, could be 0 or any number <=|V\\C|\n",
    "        if src == vc:\n",
    "            for node_in in A[src].keys():\n",
    "                orig_out = vc_out_idx[node_in]\n",
    "                if orig_out not in A.keys():\n",
    "                    A[orig_out] = {}\n",
    "                A[orig_out][node_in] = G[orig_out][node_in]\n",
    "        else:\n",
    "            #for dst in A[src]:\n",
    "            for dst in list(A[src].keys()):\n",
    "                # There must be only one in-edge to vc.\n",
    "                if dst == vc:\n",
    "                    orig_in = vc_in_idx[src]\n",
    "                    A[src][orig_in] = G[src][orig_in]\n",
    "                    del A[src][dst]\n",
    "    #del A[vc]\n",
    "    if vc in A:\n",
    "        del A[vc]\n",
    "    '''\n",
    "    try:\n",
    "        del A[vc]\n",
    "    except KeyError:\n",
    "        pass  # Do nothing if the key doesn't exist\n",
    "    '''\n",
    "\n",
    "\n",
    "    for node in C:\n",
    "        if node != orig_in:\n",
    "            src = list(mg[node].keys())[0]\n",
    "            if src not in A.keys():\n",
    "                A[src] = {}\n",
    "            A[src][node] = mg[node][src]\n",
    "\n",
    "    return A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77088d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_bangla_sentences(text):\n",
    "    # Initialize a list to store the separated sentences\n",
    "    sentences = []\n",
    "    current_sentence = \"\"\n",
    "    end_sentence_marks = ['।', '?', '!']\n",
    "    for char in text:\n",
    "        current_sentence += char\n",
    "        if char in end_sentence_marks:\n",
    "            sentences.append(current_sentence.strip())\n",
    "            current_sentence = \"\"\n",
    "    if current_sentence:\n",
    "        sentences.append(current_sentence.strip())\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c305a7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separated Sentences:\n",
      "সে ধনী কিন্তু তার ভাই গরীব।\n",
      "[('সে', 'PPR'), ('ধনী', 'NC'), ('কিন্তু', 'CSB'), ('তার', 'PPR'), ('ভাই', 'NC'), ('গরীব', 'NC'), ('।', 'PU')]\n",
      "('সে', 'PPR')\n",
      "('ধনী', 'NC')\n",
      "('কিন্তু', 'CSB')\n",
      "('তার', 'PPR')\n",
      "('ভাই', 'NC')\n",
      "('গরীব', 'NC')\n",
      "('।', 'PU')\n",
      "Transition_prob =  {1: 0.1568627450980392, 2: 0.45, 3: 1e-07, 4: 0.25, 5: 0.45, 6: 0.31794871794871793, 7: 0.15384615384615385}\n",
      "Emission_prob =  {1: 0, 2: 0, 3: 0.25, 4: 0.05, 5: 0, 6: 0, 7: 0.4838709677419355}\n",
      "['root', ('সে', 'PPR'), ('ধনী', 'NC'), ('কিন্তু', 'CSB'), ('তার', 'PPR'), ('ভাই', 'NC'), ('গরীব', 'NC'), ('।', 'PU')]\n",
      "8\n",
      "('সে', 'PPR')\n",
      "('ধনী', 'NC')\n",
      "('কিন্তু', 'CSB')\n",
      "('তার', 'PPR')\n",
      "('ভাই', 'NC')\n",
      "('গরীব', 'NC')\n",
      "('।', 'PU')\n",
      "G = {0: {1: 7.8431372549019605, 2: 0.45, 3: 125.00005, 4: 5.0, 5: 0.45, 6: 0.31794871794871793, 7: 0.6377171215880894}, 1: {2: 675.0, 3: 0.2500001, 4: 0.3, 5: 225.0, 6: 52.99145299145299, 7: 0.6377171215880894}, 2: {1: 0.1568627450980392, 3: 0.2500001, 4: 0.3, 5: 24.999999999999996, 6: 5.887939221272553, 7: 0.6377171215880894}, 3: {1: 0.1568627450980392, 2: 2.7777777777777772, 4: 0.3, 5: 0.9259259259259257, 6: 0.21807182301009453, 7: 0.6377171215880894}, 4: {1: 0.1568627450980392, 2: 0.10288065843621397, 3: 0.2500001, 5: 0.034293552812071325, 6: 0.008076734185559058, 7: 0.6377171215880894}, 5: {1: 0.1568627450980392, 2: 0.003810394756896814, 3: 0.2500001, 4: 0.3, 6: 0.0008974149095065619, 7: 0.6377171215880894}, 6: {1: 0.1568627450980392, 2: 0.000423377195210757, 3: 0.2500001, 4: 0.3, 5: 0.00014112573173691902, 7: 0.6377171215880894}, 7: {1: 0.1568627450980392, 2: 4.704191057897301e-05, 3: 0.2500001, 4: 0.3, 5: 1.568063685965767e-05, 6: 3.6930654712204196e-06}}\n",
      "DP = {0: {1: 7.8431372549019605, 3: 125.00005, 4: 5.0}, 1: {2: 675.0, 5: 225.0, 6: 52.99145299145299}, 6: {7: 0.6377171215880894}}\n",
      "List of Edges: [('root', 'সে'), ('root', 'কিন্তু'), ('root', 'তার'), ('সে', 'ধনী'), ('সে', 'ভাই'), ('সে', 'গরীব'), ('গরীব', '।')]\n",
      "List of Edges_2: [('root', 'PPR'), ('root', 'CSB'), ('root', 'PPR'), ('PPR', 'NC'), ('PPR', 'NC'), ('PPR', 'NC'), ('NC', 'PU')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from bnlp import BengaliPOS\n",
    "\n",
    "bn_pos = BengaliPOS()\n",
    "\n",
    "#bangla_text = \"আমি বাংলায় গান গাই। তুমি কি গান শুনবে? আমি ভাত খাই। সে বাজারে যায়। তিনি কি সত্যিই ভালো মানুষ? সে ধনী কিন্তু তার ভাই গরীব।\"\n",
    "#bangla_text = \"আমি ভাত খাই। সে বাজারে যায়। তিনি কি সত্যিই ভালো মানুষ? সে ধনী কিন্তু তার ভাই গরীব।\"\n",
    "def tree_generation(text):\n",
    "    bangla_text = text\n",
    "    dependency = []\n",
    "    dependency_2 = []\n",
    "    res_s = []\n",
    "    dp_edges = []\n",
    "    separated_sentences = separate_bangla_sentences(bangla_text)\n",
    "    print(\"Separated Sentences:\")\n",
    "\n",
    "    for sentence in separated_sentences:\n",
    "        print(sentence)\n",
    "        res = bn_pos.tag(sentence)\n",
    "        print(res)\n",
    "\n",
    "        transition_prob, emission_prob = greedy_decoding(res, vocab, transition_param_dict, emission_param_dict, unique_states)\n",
    "        print('Transition_prob = ', transition_prob)\n",
    "        print('Emission_prob = ', emission_prob)\n",
    "\n",
    "        res = ['root']+res\n",
    "        res_s.append(res)\n",
    "        print(res)\n",
    "\n",
    "        def get_edges(graph):\n",
    "            edges = []\n",
    "            for node in graph:\n",
    "                for neighbor in graph[node]:\n",
    "                    edges.append((node, neighbor))\n",
    "            return edges\n",
    "\n",
    "        # eita nibo\n",
    "        #num_vertices = len(res)+1\n",
    "        #print(num_vertices)\n",
    "\n",
    "        G = {}\n",
    "        dp = {}\n",
    "        print(len(res))\n",
    "        for i in range(len(res)):\n",
    "            G[i] = {}\n",
    "            if i==0:\n",
    "                p=500\n",
    "                q=100\n",
    "                r=50\n",
    "                s=1\n",
    "                for j in range(1, len(res)):\n",
    "                    print(res[j])\n",
    "                    weight = transition_prob[j] + emission_prob[j]\n",
    "                    if res[j][1] == 'CCD' or res[j][1] == 'CSB':\n",
    "                        weight = p*(transition_prob[j] + emission_prob[j])\n",
    "                        p = p*3\n",
    "                    if res[j][1] == 'VM' or res[j][1] == 'VAUX':\n",
    "                        weight = q*(transition_prob[j] + emission_prob[j])\n",
    "                        q = q/3\n",
    "                    if res[j][1] == 'PPR':\n",
    "                        weight = r*(transition_prob[j] + emission_prob[j])\n",
    "                        r = r/3\n",
    "                        \n",
    "                    else:\n",
    "                        weight = weight\n",
    "                    G[i][j] = weight # done. r kono root add korbo na\n",
    "                continue\n",
    "\n",
    "            for j in range(len(res)):\n",
    "                if (j == 0):\n",
    "                    continue\n",
    "                if (i == j):\n",
    "                    continue\n",
    "                weight = transition_prob[j] + emission_prob[j]\n",
    "                if res[j][1] == 'NC':\n",
    "                    weight = p*(transition_prob[j] + emission_prob[j])\n",
    "                    p = p/3\n",
    "                if res[j][1] == 'DAB':\n",
    "                    weight = p*(transition_prob[j] + emission_prob[j])\n",
    "                    p = p/3\n",
    "                if res[j][1] == 'NP':\n",
    "                    weight = p*(transition_prob[j] + emission_prob[j])\n",
    "                    p = p/3\n",
    "\n",
    "                G[i][j] = weight\n",
    "                \n",
    "        print(\"G =\", G)\n",
    "\n",
    "        dp = chu_liu_edmond(G, 0)\n",
    "        #dp = max_spanning_tree(G)\n",
    "        print('DP =',dp)\n",
    "        edges_of_dp = get_edges(dp)\n",
    "        dp_edges.append(get_edges(dp)) \n",
    "\n",
    "        # Define the list of tuples\n",
    "        list_of_tuples = edges_of_dp\n",
    "        word_tags = res\n",
    "        edges = []\n",
    "        for edge in list_of_tuples:\n",
    "            src = word_tags[edge[0]][0] if edge[0] != 0 else 'root'\n",
    "            dst = word_tags[edge[1]][0]\n",
    "            edges.append((src, dst))\n",
    "        print(\"List of Edges:\", edges)\n",
    "        dependency.append(edges)\n",
    "        \n",
    "        ########\n",
    "        list_of_tuples_2 = edges_of_dp\n",
    "        word_tags_2 = res\n",
    "        edges_2 = []\n",
    "        for edge in list_of_tuples_2:\n",
    "            src = word_tags[edge[0]][1] if edge[0] != 0 else 'root'\n",
    "            dst = word_tags[edge[1]][1]\n",
    "            edges_2.append((src, dst))\n",
    "        print(\"List of Edges_2:\", edges_2)\n",
    "        dependency_2.append(edges_2)\n",
    "        \n",
    "\n",
    "    return dependency, dependency_2, res_s, dp_edges\n",
    "\n",
    "#sentence = \"আমি ভাত খাই।\"\n",
    "#sentence = \"আমি ভাত খাই। আমি বাংলায় গান গাই।  সে বাজারে যায়। তুমি কি গান শুনবে?\"\n",
    "#sentence = \"প্রাকৃতিক রূপবৈচিত্র্যে ভরা আমাদের এই বাংলাদেশ। এই দেশে পরিচিত অপরিচিত অনেক পর্যটক-আকর্ষক স্থান আছে। এর মধ্যে প্রত্নতাত্ত্বিক নিদর্শন, ঐতিহাসিক মসজিদ এবং মিনার, পৃথিবীর দীর্ঘতম প্রাকৃতিক সমুদ্র সৈকত, পাহাড়, অরণ্য ইত্যাদি অন্যতম। এদেশের প্রাকৃতিক সৌন্দর্য পর্যটকদের মুগ্ধ করে। বাংলাদেশের প্রত্যেকটি এলাকা বিভিন্ন স্বতন্ত্র্র বৈশিষ্ট্যে বিশেষায়িত ।\"\n",
    "#sentence = \"পদ্মা বাংলাদেশের একটি প্রধান নদী। এটি হিমালয়ে উৎপন্ন গঙ্গানদীর প্রধান শাখা এবং বাংলাদেশের ২য় দীর্ঘতম নদী। বাংলাদেশের গুরুত্বপূর্ণ শহর রাজশাহী এই পদ্মার উত্তর তীরে অবস্থিত। পদ্মার সর্বোচ্চ গভীরতা ১,৫৭১ ফুট(৪৭৯ মিটার) এবং গড় গভীরতা ৯৬৮ফুট(২৯৫ মিটার)।\"\n",
    "sentence = \"সে ধনী কিন্তু তার ভাই গরীব।\"\n",
    "dependencies, dependencies_2, res_sies, dp_edgesies = tree_generation(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "49ab093e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('root', 'সে'), ('root', 'কিন্তু'), ('root', 'তার'), ('সে', 'ধনী'), ('সে', 'ভাই'), ('সে', 'গরীব'), ('গরীব', '।')]]\n"
     ]
    }
   ],
   "source": [
    "print(dependencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c05b0d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('root', 'PPR'), ('root', 'CSB'), ('root', 'PPR'), ('PPR', 'NC'), ('PPR', 'NC'), ('PPR', 'NC'), ('NC', 'PU')]]\n"
     ]
    }
   ],
   "source": [
    "print(dependencies_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "20d464bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(res_sies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be5b91c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (0, 3), (0, 4), (1, 2), (1, 5), (1, 6), (6, 7)]]\n"
     ]
    }
   ],
   "source": [
    "print(dp_edgesies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a4280ed3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"('root', 'RDF')\": 0.0784313725490196, \"('RDF', 'NC')\": 0.8333333333333334, \"('NC', 'NC')\": 0.31794871794871793, \"('NC', 'JJ')\": 0.06153846153846154, \"('JJ', 'NC')\": 0.7209302325581395, \"('NC', 'CCD')\": 0.03076923076923077, \"('CCD', 'NC')\": 0.47368421052631576, \"('NC', 'NV')\": 0.05128205128205128, \"('NV', 'PU')\": 0.07692307692307693, \"('NC', 'NP')\": 0.010256410256410256, \"('NP', 'JJ')\": 0.06666666666666667, \"('NC', 'VM')\": 0.16923076923076924, \"('VM', 'JQ')\": 0.017857142857142856, \"('JQ', 'JJ')\": 0.28, \"('NC', 'PP')\": 0.02564102564102564, \"('PP', 'JJ')\": 0.2, \"('VM', 'PU')\": 0.5357142857142857, \"('root', 'JQ')\": 0.0392156862745098, \"('NC', 'PPR')\": 0.05128205128205128, \"('PPR', 'NC')\": 0.45, \"('VM', 'VAUX')\": 0.19642857142857142, \"('VAUX', 'CCD')\": 0.18181818181818182, \"('CCD', 'PPR')\": 0.21052631578947367, \"('NC', 'NST')\": 0.02564102564102564, \"('NST', 'JJ')\": 0.16666666666666666, \"('VAUX', 'PU')\": 0.6363636363636364, \"('root', 'JJ')\": 0.0784313725490196, \"('NV', 'CCD')\": 0.07692307692307693, \"('NC', 'PU')\": 0.15384615384615385, \"('PU', 'NC')\": 0.45454545454545453, \"('NV', 'JJ')\": 0.15384615384615385, \"('JJ', 'VM')\": 0.11627906976744186, \"('VM', 'PRL')\": 0.017857142857142856, \"('PRL', 'AMN')\": 0.5, \"('AMN', 'NC')\": 0.42857142857142855, \"('root', 'NC')\": 0.2549019607843137, \"('NP', 'CCD')\": 0.2, \"('NST', 'NP')\": 0.16666666666666666, \"('NP', 'PU')\": 0.6, \"('PU', 'NP')\": 0.1590909090909091, \"('PU', 'PPR')\": 0.13636363636363635, \"('NC', 'CCL')\": 0.005128205128205128, \"('CCL', 'NV')\": 1.0, \"('NV', 'VM')\": 0.38461538461538464, \"('root', 'CSB')\": 0.0784313725490196, \"('CSB', 'NC')\": 0.375, \"('NC', 'JQ')\": 0.05128205128205128, \"('JQ', 'NC')\": 0.56, \"('NC', 'DAB')\": 0.005128205128205128, \"('DAB', 'NC')\": 0.625, \"('PU', 'DAB')\": 0.06818181818181818, \"('DAB', 'PU')\": 0.25, \"('NP', 'NC')\": 0.13333333333333333, \"('VM', 'CCD')\": 0.03571428571428571, \"('CCD', 'JQ')\": 0.05263157894736842, \"('JQ', 'JQ')\": 0.08, \"('JQ', 'PU')\": 0.04, \"('PU', 'JQ')\": 0.022727272727272728, \"('VM', 'CX')\": 0.05357142857142857, \"('CX', 'PU')\": 0.5714285714285714, \"('root', 'EX')\": 0.0196078431372549, \"('EX', 'MD')\": 1.0, \"('MD', 'VB')\": 1.0, \"('VB', 'NNS')\": 1.0, \"('NNS', 'VBG')\": 1.0, \"('VBG', 'WP')\": 1.0, \"('WP', 'PRP')\": 1.0, \"('PRP', 'VBD')\": 0.5, \"('VBD', '.')\": 1.0, '(\\'.\\', \"\\'\\'\")': 1.0, \"('PPR', 'JJ')\": 0.075, \"('VM', 'JJ')\": 0.017857142857142856, \"('JJ', 'PP')\": 0.023255813953488372, \"('PP', 'JQ')\": 0.2, \"('PU', 'JJ')\": 0.045454545454545456, \"('root', 'CCD')\": 0.0196078431372549, \"('PPR', 'PP')\": 0.05, \"('PP', 'PPR')\": 0.2, \"('JQ', 'VM')\": 0.04, \"('root', 'PPR')\": 0.1568627450980392, \"('PPR', 'JQ')\": 0.1, \"('VM', 'PPR')\": 0.07142857142857142, \"('root', 'ALC')\": 0.058823529411764705, \"('ALC', 'NC')\": 0.4, \"('root', 'VM')\": 0.0392156862745098, \"('PPR', 'VM')\": 0.05, \"('NC', 'ALC')\": 0.005128205128205128, \"('ALC', 'PU')\": 0.2, \"('NST', 'PU')\": 0.3333333333333333, \"('root', 'NP')\": 0.0784313725490196, \"('PU', 'CCD')\": 0.045454545454545456, \"('CCD', 'PU')\": 0.05263157894736842, \"('VM', 'NC')\": 0.017857142857142856, \"('CSB', 'DAB')\": 0.125, \"('PPR', 'AMN')\": 0.05, \"('AMN', 'VM')\": 0.42857142857142855, \"('CCD', 'JJ')\": 0.21052631578947367, \"('CSB', 'PU')\": 0.125, \"('VM', 'CSB')\": 0.017857142857142856, \"('CSB', 'PPR')\": 0.25, \"('NC', 'AMN')\": 0.015384615384615385, \"('VAUX', 'CSB')\": 0.09090909090909091, \"('JJ', 'CCD')\": 0.046511627906976744, \"('NV', 'PP')\": 0.07692307692307693, \"('PP', 'NC')\": 0.2, \"('RDF', 'PU')\": 0.16666666666666666, \"('JJ', 'NV')\": 0.023255813953488372, \"('NV', 'NC')\": 0.23076923076923078, \"('NC', 'PWH')\": 0.010256410256410256, \"('PWH', 'VM')\": 0.5, \"('NST', 'NC')\": 0.16666666666666666, \"('NC', 'RDF')\": 0.005128205128205128, \"('AMN', 'PPR')\": 0.14285714285714285, \"('PU', 'CSB')\": 0.022727272727272728, \"('root', 'DAB')\": 0.0392156862745098, \"('ALC', 'PPR')\": 0.4, \"('PPR', 'RDF')\": 0.025, \"('PP', 'VM')\": 0.2, \"('VAUX', 'NP')\": 0.09090909090909091, \"('DAB', 'JJ')\": 0.125, \"('PWH', 'PP')\": 0.25, \"('PPR', 'NST')\": 0.025, \"('NST', 'JQ')\": 0.16666666666666666, \"('JJ', 'JJ')\": 0.06976744186046512, \"('NC', 'CX')\": 0.005128205128205128, \"('CX', 'CSB')\": 0.14285714285714285, \"('CSB', 'VM')\": 0.125, \"('VM', 'AMN')\": 0.017857142857142856, \"('PPR', 'NV')\": 0.025, \"('root', 'CX')\": 0.0392156862745098, \"('PU', 'PRL')\": 0.022727272727272728, \"('PRL', 'PU')\": 0.5, \"('PPR', 'PWH')\": 0.05, \"('PWH', 'PPR')\": 0.25, \"('PU', 'ALC')\": 0.022727272727272728, \"('PPR', 'PRF')\": 0.025, \"('PRF', 'JQ')\": 1.0, \"('CX', 'CX')\": 0.14285714285714285, \"('CX', 'NC')\": 0.14285714285714285, \"('PPR', 'PU')\": 0.05, \"('PPR', 'DAB')\": 0.025, \"('root', 'PRP')\": 0.0196078431372549, \"('PRP', 'NNP')\": 0.5, \"('NNP', 'VB')\": 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(transition_param_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8e74ee51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef get_values_with_string_at_position_0(dictionary, string):\\n    values = []\\n    for key, value in dictionary.items():\\n        if key.startswith(f\"(\\'{string}\\'\"):\\n            values.append(value)\\n    return values\\n\\nstring_values = get_values_with_string_at_position_0(transition_param_dict, \\'root\\')\\nprint(string_values)\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def get_values_with_string_at_position_0(dictionary, string):\n",
    "    values = []\n",
    "    for key, value in dictionary.items():\n",
    "        if key.startswith(f\"('{string}'\"):\n",
    "            values.append(value)\n",
    "    return values\n",
    "\n",
    "string_values = get_values_with_string_at_position_0(transition_param_dict, 'root')\n",
    "print(string_values)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a04d2538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keys_and_values_with_string_at_position_0(dictionary, string):\n",
    "    key_value_tuples = []\n",
    "    for key, value in dictionary.items():\n",
    "        if key.startswith(f\"('{string}'\"):\n",
    "            key_value_tuples.append(((string, key.split(\", \")[1][1:-2]), value))\n",
    "    return key_value_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "62eae9e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "tuple_sort: (('root', 'PPR'), 0.1568627450980392)\n",
      "tuple_sort accuracy: 0.04481792717086835\n",
      "tuple_sort: (('root', 'CSB'), 0.0784313725490196)\n",
      "tuple_sort accuracy: 0.10084033613445378\n",
      "tuple_sort: (('root', 'PPR'), 0.1568627450980392)\n",
      "tuple_sort accuracy: 0.14565826330532214\n",
      "tuple_sort: (('PPR', 'NC'), 0.45)\n",
      "tuple_sort accuracy: 0.20994397759103645\n",
      "tuple_sort: (('PPR', 'NC'), 0.45)\n",
      "tuple_sort accuracy: 0.2742296918767507\n",
      "tuple_sort: (('PPR', 'NC'), 0.45)\n",
      "tuple_sort accuracy: 0.338515406162465\n",
      "tuple_sort: (('NC', 'PU'), 0.15384615384615385)\n",
      "tuple_sort accuracy: 0.4044494720965309\n",
      "tuple_sort accuracy for a sentence: 0.4044494720965309\n",
      "tuple_sort accuracy for a sentence: [0.4044494720965309]\n"
     ]
    }
   ],
   "source": [
    "def accuracy_measure(dependencies_2):\n",
    "    tuple_sort_accuracy_list = []\n",
    "    for sent in dependencies_2:\n",
    "        tuple_sort_accuracy = 0\n",
    "        print(len(sent))\n",
    "        for tuples in sent:\n",
    "            #print(tuples)\n",
    "            result = get_keys_and_values_with_string_at_position_0(transition_param_dict, tuples[0])\n",
    "            sorted_result = sorted(result, key=lambda x: x[1], reverse=True)\n",
    "            #print(sorted_result)\n",
    "            #print(len(sorted_result))\n",
    "            x = 0\n",
    "            p = 0\n",
    "            for tuple_sort in sorted_result:\n",
    "                #print(tuple_sort[0])\n",
    "                x = x+1\n",
    "                if(tuples == tuple_sort[0]):\n",
    "                    print(\"tuple_sort:\", tuple_sort)\n",
    "                    tuple_sort_accuracy = tuple_sort_accuracy + (tuple_sort[1])/len(sent)*x\n",
    "                    print(\"tuple_sort accuracy:\", tuple_sort_accuracy)\n",
    "                    p = 1\n",
    "            if (p == 0):\n",
    "                print(\"sorted result:\", sorted_result)\n",
    "                tuple_sort_accuracy = tuple_sort_accuracy + len(sorted_result)* .02\n",
    "                print(\"tuple_sort accuracyyyy:\", tuple_sort_accuracy)\n",
    "        tuple_sort_accuracy_list.append(tuple_sort_accuracy)\n",
    "        print(\"tuple_sort accuracy for a sentence:\", tuple_sort_accuracy)\n",
    "    #print(\"tuple_sort accuracy for a sentence:\", tuple_sort_accuracy_list)\n",
    "    return tuple_sort_accuracy_list\n",
    "print(\"tuple_sort accuracy for a sentence:\", accuracy_measure(dependencies_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9f2f0ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "tuple_sort: (('root', 'JQ'), 0.0392156862745098)\n",
      "tuple_sort accuracy: 1\n",
      "tuple_sort: (('NC', 'PU'), 0.15384615384615385)\n",
      "tuple_sort accuracy: 2\n",
      "tuple_sort: (('NP', 'JJ'), 0.06666666666666667)\n",
      "tuple_sort accuracy: 3\n",
      "tuple_sort: (('NP', 'NC'), 0.13333333333333333)\n",
      "tuple_sort accuracy: 4\n",
      "tuple_sort accuracy for a sentence: 0.6666666666666666\n",
      "12\n",
      "tuple_sort: (('root', 'PPR'), 0.1568627450980392)\n",
      "tuple_sort accuracy: 1\n",
      "tuple_sort: (('root', 'VM'), 0.0392156862745098)\n",
      "tuple_sort accuracy: 2\n",
      "tuple_sort: (('root', 'CCD'), 0.0196078431372549)\n",
      "tuple_sort accuracy: 3\n",
      "tuple_sort: (('PU', 'JJ'), 0.045454545454545456)\n",
      "tuple_sort accuracy: 4\n",
      "tuple_sort: (('PU', 'JJ'), 0.045454545454545456)\n",
      "tuple_sort accuracy: 5\n",
      "tuple_sort: (('PU', 'JJ'), 0.045454545454545456)\n",
      "tuple_sort accuracy: 6\n",
      "tuple_sort: (('PPR', 'NC'), 0.45)\n",
      "tuple_sort accuracy: 7\n",
      "tuple_sort: (('PPR', 'NC'), 0.45)\n",
      "tuple_sort accuracy: 8\n",
      "tuple_sort: (('PPR', 'NC'), 0.45)\n",
      "tuple_sort accuracy: 9\n",
      "tuple_sort: (('NC', 'PU'), 0.15384615384615385)\n",
      "tuple_sort accuracy: 10\n",
      "tuple_sort accuracy for a sentence: 0.8333333333333334\n",
      "10\n",
      "tuple_sort: (('root', 'JJ'), 0.0784313725490196)\n",
      "tuple_sort accuracy: 1\n",
      "tuple_sort: (('NC', 'JJ'), 0.06153846153846154)\n",
      "tuple_sort accuracy: 2\n",
      "tuple_sort: (('NP', 'NC'), 0.13333333333333333)\n",
      "tuple_sort accuracy: 3\n",
      "tuple_sort: (('NP', 'NC'), 0.13333333333333333)\n",
      "tuple_sort accuracy: 4\n",
      "tuple_sort: (('NP', 'NC'), 0.13333333333333333)\n",
      "tuple_sort accuracy: 5\n",
      "tuple_sort: (('NP', 'NC'), 0.13333333333333333)\n",
      "tuple_sort accuracy: 6\n",
      "tuple_sort: (('NC', 'JJ'), 0.06153846153846154)\n",
      "tuple_sort accuracy: 7\n",
      "tuple_sort accuracy for a sentence: 0.7\n",
      "20\n",
      "tuple_sort: (('root', 'NC'), 0.2549019607843137)\n",
      "tuple_sort accuracy: 1\n",
      "tuple_sort: (('root', 'CCD'), 0.0196078431372549)\n",
      "tuple_sort accuracy: 2\n",
      "tuple_sort: (('NC', 'NC'), 0.31794871794871793)\n",
      "tuple_sort accuracy: 3\n",
      "tuple_sort: (('NC', 'NC'), 0.31794871794871793)\n",
      "tuple_sort accuracy: 4\n",
      "tuple_sort: (('NC', 'NC'), 0.31794871794871793)\n",
      "tuple_sort accuracy: 5\n",
      "tuple_sort: (('NC', 'NC'), 0.31794871794871793)\n",
      "tuple_sort accuracy: 6\n",
      "tuple_sort: (('NC', 'NC'), 0.31794871794871793)\n",
      "tuple_sort accuracy: 7\n",
      "tuple_sort: (('NC', 'NC'), 0.31794871794871793)\n",
      "tuple_sort accuracy: 8\n",
      "tuple_sort: (('NC', 'NC'), 0.31794871794871793)\n",
      "tuple_sort accuracy: 9\n",
      "tuple_sort accuracy for a sentence: 0.45\n",
      "tuple_sort accuracy for a sentence: [0.6666666666666666, 0.8333333333333334, 0.7, 0.45]\n"
     ]
    }
   ],
   "source": [
    "def accuracy_measure_2(dependencies_2):\n",
    "    tuple_sort_accuracy_list = []\n",
    "    for sent in dependencies_2:\n",
    "        tuple_sort_accuracy = 0\n",
    "        len_sen = len(sent)\n",
    "        print(len(sent))\n",
    "        tuple_sort_accuracy = 0\n",
    "        for tuples in sent:\n",
    "            #print(tuples)\n",
    "            result = get_keys_and_values_with_string_at_position_0(transition_param_dict, tuples[0])\n",
    "            sorted_result = sorted(result, key=lambda x: x[1], reverse=True)\n",
    "            #print(sorted_result)\n",
    "            #print(len(sorted_result))\n",
    "            x = 0\n",
    "            for tuple_sort in sorted_result:\n",
    "                #print(tuple_sort[0])\n",
    "                x = x+1\n",
    "                if(tuples == tuple_sort[0]):\n",
    "                    print(\"tuple_sort:\", tuple_sort)\n",
    "                    tuple_sort_accuracy = tuple_sort_accuracy + 1\n",
    "                    print(\"tuple_sort accuracy:\", tuple_sort_accuracy)\n",
    "                    \n",
    "        tuple_sort_accuracy = tuple_sort_accuracy/len_sen\n",
    "        tuple_sort_accuracy_list.append(tuple_sort_accuracy)\n",
    "        print(\"tuple_sort accuracy for a sentence:\", tuple_sort_accuracy)\n",
    "    #print(\"tuple_sort accuracy for a sentence:\", tuple_sort_accuracy_list)\n",
    "    return tuple_sort_accuracy_list\n",
    "print(\"tuple_sort accuracy for a sentence:\", accuracy_measure_2(dependencies_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7291f3b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"('RDF', '<unk>')\": 1.0, \"('NC', '<unk>')\": 0.9076923076923077, \"('JJ', 'বিরোধী')\": 0.046511627906976744, \"('CCD', 'এবং')\": 0.631578947368421, \"('NV', 'করা')\": 0.23076923076923078, \"('PU', '।')\": 0.4838709677419355, \"('NC', 'সালে')\": 0.010256410256410256, \"('NP', '<unk>')\": 0.8, \"('JJ', 'রাজনৈতিক')\": 0.046511627906976744, \"('JJ', '<unk>')\": 0.6511627906976745, \"('VM', '<unk>')\": 0.7142857142857143, \"('JQ', '<unk>')\": 0.48, \"('PP', '<unk>')\": 0.6, \"('VM', 'করে')\": 0.08928571428571429, \"('JQ', 'একটি')\": 0.24, \"('NC', 'স্থান')\": 0.010256410256410256, \"('PPR', 'আমাদের')\": 0.1, \"('NC', 'জীবনের')\": 0.010256410256410256, \"('VAUX', '<unk>')\": 0.8181818181818182, \"('PPR', 'আমরা')\": 0.075, \"('NST', 'মধ্যে')\": 0.3333333333333333, \"('JJ', 'বিভিন্ন')\": 0.06976744186046512, \"('NV', 'চালানো')\": 0.15384615384615385, \"('CCD', 'ও')\": 0.3157894736842105, \"('PU', '-')\": 0.06451612903225806, \"('PRL', '<unk>')\": 0.5, \"('AMN', '<unk>')\": 1.0, \"('NP', 'আম')\": 0.06666666666666667, \"('PU', ',')\": 0.3333333333333333, \"('PPR', 'তারা')\": 0.05, \"('CCL', '<unk>')\": 1.0, \"('NV', 'পাওয়া')\": 0.15384615384615385, \"('CSB', 'কিন্তু')\": 0.25, \"('DAB', 'এ')\": 0.625, \"('NP', 'সি')\": 0.13333333333333333, \"('JQ', 'প্রথম')\": 0.08, \"('CX', 'না')\": 0.5714285714285714, \"('EX', '<unk>')\": 1.0, \"('MD', '<unk>')\": 1.0, \"('VB', '<unk>')\": 1.0, \"('NNS', 'ও')\": 1.0, \"('VBG', '<unk>')\": 1.0, \"('WP', '<unk>')\": 1.0, \"('PRP', '<unk>')\": 0.5, \"('VBD', '<unk>')\": 1.0, \"('.', '<unk>')\": 1.0, '(\"\\'\\'\", \\'<unk>\\')': 1.0, \"('JQ', 'প্রতি')\": 0.12, \"('PPR', '<unk>')\": 0.25, \"('JJ', 'বৃত্তিমূলক')\": 0.046511627906976744, \"('NC', 'শিক্ষার')\": 0.010256410256410256, \"('JJ', 'প্রাথমিক')\": 0.046511627906976744, \"('NV', '<unk>')\": 0.3076923076923077, \"('VM', 'হবে')\": 0.07142857142857142, \"('CCD', '<unk>')\": 0.05263157894736842, \"('PPR', 'আমার')\": 0.075, \"('PPR', 'আমি')\": 0.1, \"('ALC', '<unk>')\": 0.8, \"('PU', '!')\": 0.043010752688172046, \"('VM', 'যাও')\": 0.017857142857142856, \"('ALC', 'কোথায়')\": 0.2, \"('PU', '?')\": 0.03225806451612903, \"('PPR', 'তিনি')\": 0.075, \"('VM', 'ছিলেন')\": 0.03571428571428571, \"('PPR', 'তার')\": 0.05, \"('PPR', 'তাকে')\": 0.05, \"('NST', 'পর')\": 0.3333333333333333, '(\\'PU\\', \"\\'\")': 0.043010752688172046, \"('NC', 'আম')\": 0.005128205128205128, \"('CSB', '<unk>')\": 0.5, \"('DAB', 'এই')\": 0.25, \"('VAUX', 'হবে')\": 0.18181818181818182, \"('NC', 'প্রয়োজনীয়তা')\": 0.010256410256410256, \"('PPR', 'এটা')\": 0.05, \"('CSB', 'যে')\": 0.25, \"('NST', '<unk>')\": 0.3333333333333333, \"('NC', 'চরিত্রটি')\": 0.010256410256410256, \"('NV', 'করার')\": 0.15384615384615385, \"('PP', 'জন্য')\": 0.3, \"('PWH', 'কি')\": 1.0, \"('JJ', 'শুকনো')\": 0.046511627906976744, \"('PPR', 'তোমাকে')\": 0.05, \"('NC', 'ধ্বংস')\": 0.010256410256410256, \"('VM', 'করবে')\": 0.03571428571428571, \"('PPR', 'তোমার')\": 0.075, \"('DAB', '<unk>')\": 0.125, \"('PP', 'করে')\": 0.1, \"('JQ', 'একটা')\": 0.08, \"('VM', 'ছিল')\": 0.03571428571428571, \"('JJ', 'ছোট')\": 0.046511627906976744, \"('CX', 'কি')\": 0.2857142857142857, \"('NC', 'কোথায়')\": 0.005128205128205128, \"('PRL', 'যাও')\": 0.5, \"('PRF', '<unk>')\": 1.0, \"('CX', '<unk>')\": 0.14285714285714285, \"('NC', 'জাতি')\": 0.010256410256410256, \"('PRP', 'আমি')\": 0.5, \"('NNP', '<unk>')\": 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(emission_param_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5b1fe13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RDF', 'NC', 'JJ', 'CCD', 'NV', 'PU', 'NP', 'VM', 'JQ', 'PP', 'PPR', 'VAUX', 'NST', 'PRL', 'AMN', 'CCL', 'CSB', 'DAB', 'CX', 'EX', 'MD', 'VB', 'NNS', 'VBG', 'WP', 'PRP', 'VBD', '.', \"''\", 'ALC', 'PWH', 'PRF', 'NNP']\n"
     ]
    }
   ],
   "source": [
    "print(unique_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06b34f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install Flask flask-ngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e1edcd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template, request\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Example NLP function\n",
    "def analyze_text(text):\n",
    "    # Replace this with your actual NLP processing logic\n",
    "    # For demonstration, let's just return the input text\n",
    "    dependencies, dependencies_2, res_sies, dp_edgesies = tree_generation(text)\n",
    "    accuracy = accuracy_measure(dependencies_2)\n",
    "    return {\"title\": \"For Bangla\", \"sentence\": text, \"pos\": res_sies, \"input\": dependencies, \"num\": dp_edgesies, \"accu\": accuracy}\n",
    "\n",
    "@app.route(\"/\", methods=['GET', 'POST'])\n",
    "def index():\n",
    "    if request.method == 'POST':\n",
    "        user_input = request.form['user_input']\n",
    "        analysis = analyze_text(user_input)\n",
    "        return render_template('new_result.html', analysis=analysis)\n",
    "    return render_template('new.html')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b28847cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(json_file_path):\n",
    "    with open(json_file_path, 'r', encoding=\"utf-8\") as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    formatted_data = []\n",
    "\n",
    "    for entry in data:\n",
    "        sentence_tokens = entry['sentence']\n",
    "        labels = entry['labels']\n",
    "\n",
    "        tokens_with_index = []\n",
    "\n",
    "        for index, (word, tag) in enumerate(zip(sentence_tokens, labels)):\n",
    "            tokens_with_index.append(f\"{index}\\t{word}\\t{tag}\")\n",
    "\n",
    "        sentence = '\\n'.join(tokens_with_index)\n",
    "        formatted_data.append(sentence)\n",
    "\n",
    "    return '\\n\\n'.join(formatted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ea2a0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on dev data with greedy decoding = 0.7692307692307693\n"
     ]
    }
   ],
   "source": [
    "def greedy_decoding_accuracy(formatted_data, vocab, transition_param_dict, emission_param_dict, state_track):\n",
    "    accuracy = []\n",
    "\n",
    "    for line in formatted_data.split('\\n\\n'):\n",
    "        prior_st = 'root'\n",
    "        for ent in line.split('\\n'):\n",
    "            index, word, st = ent.split('\\t')\n",
    "            if word not in vocab.keys():\n",
    "                word = '<unk>'\n",
    "\n",
    "            max_prob = 0\n",
    "            max_st = None\n",
    "\n",
    "            for st_ in state_track:\n",
    "                tr = transition_param_dict.get(str((prior_st, st_)), 1e-7)\n",
    "                em = emission_param_dict.get(str((st_, word)), 0)\n",
    "                prob = tr * em\n",
    "\n",
    "                if prob > max_prob:\n",
    "                    max_prob = prob\n",
    "                    max_st = st_\n",
    "\n",
    "            accuracy.append(st == max_st)\n",
    "            prior_st = max_st\n",
    "            #print(prior_st)\n",
    "\n",
    "    return sum(accuracy) / len(accuracy)\n",
    "\n",
    "formatted_dev_data = format_data('C:/Users/Admin/Desktop/Thesis/POS Tagging/dev_bangla.json')\n",
    "accuracy = greedy_decoding_accuracy(formatted_dev_data, vocab, transition_param_dict, emission_param_dict, unique_states)\n",
    "print(\"Accuracy on dev data with greedy decoding =\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ab7bc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
